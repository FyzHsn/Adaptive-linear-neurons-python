---
title: "Notes on [Ada]ptive [Li]near [Ne]uron (ADALINE) algorithms"
author: "Faiyaz Hasan"
date: "July 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Adaline was developed by Bernard Widrow and Ted Hoff at Stanford University during 1960. The algorithm is generally considered an improvement over the Perceptron. Yet, what surprises my is that it is much more intuitive than the latter algorithm. The back-and-forth rotation of the weight vector and its eventual convergence is surprising to me. The conceptual intuitiveness of the algorithm comes from the usage of a convex, differentiable cost function that is to be minimized. The divergence/convergence of the weight factors for high/low learning rates is obvious.

In this document, we will play with two variants, the Gradient Descent and the Stochastic Gradient Descent method. Other stategies include marrying the two previous algorithms in something called the mini-batch algorithms.

Adaline - Gradient Descent
--------------------------

### Iris Dataset and preprocessing

We're interested in both the regular iris dataset as well as the standardized version with the mean subtracted out and the standard deviation reduced to 1.

```{r, iris preprocessing}
# load the iris dataset
data(iris)
str(iris)

# sepal and petal dimensions
X <- iris[1:100, 1:4]
names(X) <- tolower(names(X))

# initialize X_std dataframe
X_std <- iris[1:100, 1:4]

# define standardized data set
for (i in 1:4) {
        X_std[, i] <- (X[, i]-mean(X[, i]))/sd(X[, i])
}

# first few rows of both data frames
head(X)
head(X_std)

# binary classification vector according to species: setosa(-1), versicolor(1)
y <- rep(1, 100)
y[which(iris[, 5] == "setosa")] <- -1
y[which(iris[, 5] == "versicolor")] <- 1

# pre-process non-separable data points (ns stands for non-separable)
# sepal and petal dimensions
Xns <- iris[51:150, 1:4]
names(Xns) <- tolower(names(Xns))

# initialize X_std dataframe
Xns_std <- iris[51:150, 1:4]

# define standardized data set
for (i in 1:4) {
        Xns_std[, i] <- (Xns[, i]-mean(Xns[, i]))/sd(Xns[, i])
}

# binary classification vector according to species: setosa(-1), versicolor(1)
yns <- rep(1, 100)
l1 <- sum(iris[, 5] == "versicolor")
l2 <- sum(iris[, 5] == "virginica")
yns[1:l1] <- -1
yns[(l1+1):(l1+l2)] <- 1
dim(Xns)
length(yns)
```

### Algorithm

Now, let us implement the adaline gradient descent algorithm. We're interested in the update of the weights, cost function and the amount of errors made per epoch. In the adaline, the weights are updated over entire data set in one go.

```{r, adalineGD algorithm}
adalineGD <- function(X, y, n_iter=10, eta=0.01) {
        
        # extend input vector and initialize extended weight
        X[, dim(X)[2] + 1] <- 1 
        X <- as.matrix(X)
        w <- as.matrix(rep(0, dim(X)[2]))
        
        # initialize cost values - gets updated according to epochnums -                number of epochs
        cost <- rep(0, n_iter)
        error <- rep(0, n_iter)
        
        # loop over the number of epochs
        for (i in 1:n_iter) {
                
                # find the number of wrong prediction before weight update
                for (j in 1:dim(X)[1]) {
                        
                        # compute net input
                        z <- sum(w * X[j, ])
                        
                        # quantizer
                        if(z < 0.0) {
                                ypred <- -1
                        } else {
                                ypred <- 1
                        }
                        
                        # comparison with actual labels and counting error
                        if(ypred != y[j]) {
                                error[i] <- error[i] + 1
                        }
                }
                cost[i] <- sum((y - X %*% w)^2)/2
                
                # update weight according to gradient descent
                w <- w + eta*t(X) %*% (y - X %*% w)
        }
        
        # data frame consisting of cost and error info
        infomatrix <- matrix(rep(0, 3 * n_iter), nrow = n_iter, ncol = 3)
        infomatrix[, 1] <- 1:n_iter
        infomatrix[, 2] <- log(cost)
        infomatrix[, 3] <- error
        
        infodf <- as.data.frame(infomatrix)
        names(infodf) <- c("epoch", "log(cost)", "error")
        
        return(infodf)
}
```

* It would be cool to see how the cost function and the error is updated
* Comparison of regular and standard data
* testing different learning rates
* Difference with perceptron: convergence depends on learning rate for adaline. Perceptron convergence depends on linear separability, meanhwile for the adaline it can (???) converge without separability. Test it with the versicolor vs virginica dataset.

```{r, standard vs non-standard separable data}
library(ggplot2)
library(reshape2)

# Standardized vs non-standard data: Cost and error function
eta <- 0.0001
n_iter <- 100
result1 <- adalineGD(X, y, n_iter, eta)
label <- rep("non-standard", dim(result1)[1])
result1 <- cbind(label, result1)
result2 <- adalineGD(X_std, y, n_iter, eta)
label <- rep("standard", dim(result2)[1])
result2 <- cbind(label, result2)

df <- rbind(result1, result2)

# long format of data frame
dflong <- melt(df, id.vars=c("epoch", "label"))
head(dflong)

ggplot(dflong, aes(x=epoch, y=value)) + 
        geom_line(aes(color=label, linetype=label), size = 1) +
        facet_grid(variable ~ .) + xlab("Epoch #") + ylab("") +
        ggtitle("Cost and error function for a dataset \n and its standardized form: eta = 0.0001")
```

This makes me curious as to the convergence properties based on different learning rates. That is what we will look at next. Furthermore, we will focus on non-standard and standard data

```{r, cost and error function dependence on learning rate}
library(ggplot2)
library(reshape2)

# set the number of sweeps through the entire data set 
n_iter <- 50

# the list of learning rates that interests me
eta <- c("0.00005", "0.0001", "0.00025", "0.0005")

# initialize data frames before looping
# adaline applied before standardizing data
result <- adalineGD(X, y, n_iter, as.numeric(eta[1]))
learnrate <- rep(eta[1], dim(result)[1])
result <- cbind(learnrate, result)

# adaline applied after standardizing data
result_std <- adalineGD(X_std, y, n_iter, as.numeric(eta[1]))
learnrate_std <- rep(eta[1], dim(result_std)[1])
result_std <- cbind(learnrate_std, result_std)

# updating the dataframe by row binding cost and error function for each new learning rate
df <- result
df_std <- result_std

# Standardized vs non-standard data: Cost and error function
for(i in 2:length(eta)) {
        
        # adaline applied before standardizing data
        result <- adalineGD(X, y, n_iter, as.numeric(eta[i]))
        learnrate <- rep(eta[i], dim(result)[1])
        result <- cbind(learnrate, result)
        
        # adaline applied after standardizing data
        result_std <- adalineGD(X_std, y, n_iter, as.numeric(eta[i]))
        learnrate_std <- rep(eta[i], dim(result_std)[1])
        result_std <- cbind(learnrate_std, result_std)
        
        # updating the dataframe by row binding cost and error function for each new learning rate
        df <- rbind(df, result)
        df_std <- rbind(df_std, result_std)
}

# long format of data frame
dflong <- melt(df, id.vars=c("epoch", "learnrate"))
df_stdlong <- melt(df_std, id.vars=c("epoch", "learnrate_std"))

head(dflong)
head(df_stdlong)

g <- ggplot(dflong, aes(x=epoch, y=value)) +
        geom_line(aes(color = learnrate, linetype = learnrate), size = 1) +
        facet_grid(variable ~ ., scales="free") + xlab("Epoch #") + ylab("") +
        ggtitle("Cost and error function convergence for \n varying learning rates")
head(df_stdlong)

g_std <- ggplot(df_stdlong, aes(x=epoch, y=value)) +
        geom_line(aes(color = learnrate_std, linetype = learnrate_std), size = 1) +
        facet_grid(variable ~ ., scales="free") + xlab("Epoch #") + ylab("") +
        ggtitle("Cost and error function convergence for \n varying learning rates - standardized data")

# print plots
print(g)
print(g_std)
```

Wow! Those were great plots to look at because I was forced to think about several new things:

1. Convergence is not permanent.  
2. At a certain minimum cost function point, depending on the learning rate, we need to stop updating the weights or it'll start increasing.  
3. Increasing the learning rate (small enough to converge over the range of epochs) can cause the cost function for the standard and the non-standard data to converge.  
4. The last plot where we begin to decrease performance of the algorithm applied to the non-standard data, it still performs fine for the standardized data set.

From this plot, we can see that the cost function approaches the minimum faster as well as the error function reducing to nil quicker in the case of the standardized data.

Now, let us look at what happens when we run the adalineGD for non-separable data. This is the same dataset we also applied the Perceptron on and found that the minimum attainable error was 2.

```{r, standard vs non-standard, non-separable}
# Standardized vs non-standard data: Cost and error function
eta <- 0.0001
n_iter <- 300
result1 <- adalineGD(Xns, yns, n_iter, eta)
label <- rep("non-standard", dim(result1)[1])
result1 <- cbind(label, result1)
result2 <- adalineGD(Xns_std, yns, n_iter, eta)
label <- rep("standard", dim(result2)[1])
result2 <- cbind(label, result2)

df <- rbind(result1, result2)

# long format of data frame
dflong <- melt(df, id.vars=c("epoch", "label"))
head(dflong)

ggplot(dflong, aes(x=epoch, y=value)) + 
        geom_line(aes(color=label, linetype=label), size = 1) +
        facet_grid(variable ~ ., scales = "free") + xlab("Epoch #") + ylab("") +
        ggtitle("Cost and error function for a non-separable dataset \n and its standardized form: eta = 0.0001")
```

* Interestingly enough, we see that the convergence of the cost function of the non-separable standard vs non-standard data is much bigger.  
* The convergence of the errors is also much slower than in the previous case (10 vs 200 epochs).  
*For the standardized data, there is a sharp drop in the number of errors as it was in the separable data.  
* And then additional accuracy seems to take longer and longer.  
*One question that I have is if there can be a resurgence in the number of errors or not.  
* This also makes me curious as to how the stochastic gradient descent algorithm will perform.  

**Moral: (i) Standardize data set (ii) Stop updating weights once cost function begins to increase**



Adaline - Stochastic Gradient Descent
-------------------------------------

